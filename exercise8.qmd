---
title: "exercise 8"
format: html
editor: visual
---

# Practice Simple Linear Regression

For this exercise I will be doing a simple linear regression examining how brain sizes (ECV) of different primate taxa varies as a function of group size

## Step 1: Libraries and Data

First I will get the data loaded in an do some initial analysis using the skim() function in the {skimr} package to generate the median, minimum, maximum, 1st and 3rd quartile values, mean, and standard deviation for each variable.

```{r}
#| message: false
#| warning: false
library(tidyverse)
library(skimr)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/Street_et_al_2017.csv"
d <- read_csv(f)
skim(d)
```

## Step 2: Initial Plots

Next I will produce plots which plot brain size (ECV) as a function of group size, longevity, juvenile period length, and reproductive lifespan. I used the {cowplot} package to arrange these 4 plots in a grid

```{r}
#| message: false
#| warning: false
library(ggplot2)
library(cowplot)
ECV_GS <- ggplot(data = d, mapping = aes(Group_size, ECV)) +
                   geom_point()
ECV_Long <- ggplot(data = d, mapping = aes(Longevity, ECV)) +
                   geom_point()
ECV_Wean <- ggplot(data = d, mapping = aes(Weaning, ECV)) +
                   geom_point()
ECV_Repro <- ggplot(data = d, mapping = aes(Repro_lifespan, ECV)) +
                   geom_point()
plot_grid(ECV_GS, ECV_Long, ECV_Wean, ECV_Repro)
```

## Step 3: ECV Beta 1 and Beta 0

Now I will calculate the Beta 1 and Beta 0 (least squares regression coefficiants) for ECV (brain size) as a function of social group size. Since there are NA values in the data set, I filtered out any NAs in both ECV and Group_size before calculating the coefficients.

```{r}
#| message: false
#| warning: false
#Cleaning data to remove NA values from variables of interest
d <- d |> 
  filter(!is.na(ECV) & !is.na(Group_size))

ECV_sd <- sd(d$ECV) #Y
GS_sd <- sd(d$Group_size) #X

b1 <- cor(d$Group_size, d$ECV) * (ECV_sd/GS_sd)
b0 <- mean(d$ECV) - (b1*mean(d$Group_size))
print(paste("Beta 1 =",b1, "; Beta 0 =",b0))
```

## Step 4: Confirm Betas

Now I will use the lm() function to recalculate the coefficient to confirm that I calculated them properly by hand.

```{r}
#| message: false
#| warning: false
ECVlm <- lm(ECV ~ Group_size, data = d)
print(ECVlm)
```

## Step 5: Rinse and Repeat

Now I will again measure the regression coefficients but this time I will do them based on each of the three radiations of primates. To do this by hand, I first created a function called regCof() which will take the data set, name of the radiation, and the x and y values the user want to run a regression on as arguments. The function will then calculate the Beta 1 and Beta 0 coefficients. To confirm these results, the filtLM() function I created will take the same agreements and apply the lm() function to the filtered data set.

All three groups of primates have different values for both of the regression coefficients, with platyrrhines as having the highest Beta 1 value and catarrhines having the highest Beta 0 value. All of them however have positive coefficients with slope being greater than 1, suggesting a similar positive relationship between brain size and group size across all three primate radiations.

```{r}
#| message: false
#| warning: false
#Calc b1 and b0 on filtered dataset by radiation
regCof <- function(df, rad, x, y){
  group <- df |>
    filter(Taxonomic_group == rad)
  sdX <- sd(group[[x]])
  sdY <- sd(group[[y]])
  b1 <- cor(group[[x]], group[[y]], use = "complete.obs") * (sdY/sdX)
  b0 <- mean(group[[y]]) - (b1*mean(group[[x]]))
  return(c(b1, b0))
}

#Use lm() on a filtered data set
filtLM <- function(df, rad, x, y){
  group <- df |>
    filter(Taxonomic_group == rad)
  ECVlm <- lm(group[[y]] ~ group[[x]])
  return(ECVlm)
}

cattys <- regCof(d, "Catarrhini", "Group_size", "ECV")
cattysLM <- filtLM(d, "Catarrhini", "Group_size", "ECV")
print(cattys)
print(cattysLM)

platys <- regCof(d, "Platyrrhini", "Group_size", "ECV")
platysLM <- filtLM(d, "Platyrrhini", "Group_size", "ECV")
print(platys)
print(platysLM)

strepys <- regCof(d, "Strepsirhini", "Group_size", "ECV")
strepysLM <- filtLM(d, "Strepsirhini", "Group_size", "ECV")
print(strepys)
print(strepysLM)
```

## Step 6: ECV Stats

Now, I will use my first slope coefficient (Beta 1) based off the first regression of brain size and group size to calculate the standard error, 95% confidence intervals, and p-value associated with the coefficient.

In order to calculate the standard error, I first need to determine the sum of squares of X (SSX) and mean of remaining variance (MSE). SSX is calculated by taking the sum of the values for out X value in the model section of the linear model minus the mean of this value squared. To calculate the MSE, we need to divide the squared sum of the residuales by the number of degrees of freedom for the error sum of squares. Once we have the MSE and SSX we divide them and take the square root of that to determine the standard error of the slope coefficient

For the 95% confidence intervals we add the slope coefficient to a qt() function based on the size of the confidence interval and out degrees of freedom and multiplied by the standard error we calculated.

Finally, for the p-value, we first calculate the t-statistics (slope coefficient/standard error) and then plugging it into the pt() function which we then multiply by 2. As a final step, I used the summary() function on my linear model to check my work for the standard error and p-value and the confint() function on the linear model to make sure I calculated the confidence intervals correctly.

```{r}
#Step 6.1: Calculate MSE and SSX
SSE <- sum(ECVlm$residuals^2)
SSX <- sum((ECVlm$model$Group_size - mean(ECVlm$model$Group_size))^2)
df_error <- nrow(d) - 2
MSE <- SSE/df_error

#Step 6.2: Calc SE of b1
SEb1 <- sqrt(MSE/SSX)
SEb1
#Step 6.3: 95% CI for b1
CIb1 <- b1 + qt(p=c(0.025, 0.975), ncp=0, df=150)*SEb1
CIbi
#Step 6.4: p-value of b1, need t_stat (t = estimate/SE) to calc
#p-value (p = 2*pt(t, df = nrow - 2))
tB1 <- b1/SEb1
pB1 <- 2*pt(tB1, df = 149, lower.tail = FALSE)
tB1
pB1
#Step 6.4: Compare results to those of the lm() function
summary(ECVlm)
lmCI <- confint(ECVlm)
lmCI

```

## Step 7: Permutation Time

Now we will use a for{} loop to run 1000 permutations generate a null sampling distribution for the slope coefficient. We need to permute the brain size variable (ECV) as we are interested in how it changes in relation to group size. In the loop, we will create a temporary data frame based on the original to ensure the integrity of the data. The we will sample the temporary data for brain size and use this sample to run an ordinary least squares regression and save the slope coefficient in a vector called slopes.

After the permutation, we will calculate the p-value. The original p-value was 7.26e-11 based on the lm() function. We will calculate the p-value by hand using our permuted data via the theory-based method.

```{r}
#Permute
reps <- 1000
slopes <- vector()

for(i in 1:reps){
  temp <- d #temporary dataframe to hold while we permute, maintain data integrity
  temp$ECV <- sample(temp$ECV) #Sample ECV values
  m <- lm(ECV ~ Group_size, data = temp) #Create lm based on shuffled temp values
  slopes[[i]] <- m$coefficients[[2]] #store slopes
} #Perm vector is dist in differnces in homerange size after each reshuffling

#Perm dist should be centerd on 0
hist(slopes)

#Now we estimate the p value using the theory based method
#This method requires the standard error of our null sampling distribution (standard
#deviation), 
slopes_sd <- sd(slopes)
slopes_mean <- mean(slopes)
t <- (slopes_mean - b1)/slopes_sd
p_upper <- 1 - pt(abs(t), df = 150)
p_lower <- pt(-1 * abs(t), df = 150)
p <- p_upper + p_lower
print(p) #Slightly larger but still significant p-value
summary(ECVlm) #OG slope p-value is 7.26e-11
```

## Step 8: CI by Bootstraping

```{r}
n_boot <- 1000
boot <- vector(length=n_boot) #set up dummy variable to hold our sims
n <- length(d) #boostrap sample size will be same length of data
for (i in 1:n_boot){
  bootSamp <- slice_sample(d, n = n, replace = TRUE)
  bootM <- lm(ECV ~ Group_size, data = bootSamp)
  boot[[i]] <- bootM$coefficients[[2]]
}
#CI by quantile
ci <- quantile(boot, probs = c(0.025, 0.975))

#CI by theory based method
percent_ci <- 95
alpha <- 1 - percent_ci/100
bootMean <- mean(boot)
bootSE <- sd(boot)
lowerCI <- bootMean + qnorm(alpha/2)*bootSE
upperCI <- bootMean + qnorm(1 - alpha/2)*bootSE
ciTheory <- c(lowerCI, upperCI)

```
